{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "# !pip install nltk\n",
    "# import docx2txt\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "import re\n",
    "import subprocess\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text Data From PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    return text\n",
    "\n",
    "txt = extract_text_from_pdf(\"Rajat's Resume (1)..pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajat Agarwaal\n",
      "Data Scientist\n",
      "Skilled Data Scientist with 2.5 years of experience executing data driven solutions to increase eﬃciency,\n",
      "accuracy and utility of internal data processing. Experienced at creating regression models, classiﬁcation\n",
      "models  using  predictive  modelling,  Computer  Vision  and  analyzing  data  mining  algorithms  to  deliver\n",
      "insights and implement action oriented solutions to complex business problems.\n",
      "\n",
      "rajatagarwaal30@gmail.com\n",
      "\n",
      "Ghaziabad (NCR Region), India\n",
      "\n",
      "linkedin.com/in/rajat-agarwaal\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Machine Learning Engineer\n",
      "AgEYE Technologies\n",
      "01/2021 - Present, \n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "09958168687\n",
      "\n",
      "30 March, 1994\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Python\n",
      "\n",
      "C\n",
      "\n",
      "Machine Learning\n",
      "\n",
      "Deep Learning\n",
      "\n",
      "Bangalore, India\n",
      "\n",
      "Computer Vision\n",
      "\n",
      "Natural Language Processing\n",
      "\n",
      "SQL\n",
      "\n",
      "Pig\n",
      "\n",
      "Hive\n",
      "\n",
      "Basics of HDFS\n",
      "\n",
      "Undertaking data collection, preprocessing and analysis\n",
      "\n",
      "Building models to address business problems\n",
      "\n",
      "Statistical Analysis\n",
      "\n",
      "Gitlab\n",
      "\n",
      "Tableau\n",
      "\n",
      "Propose solutions and strategies to business challenges\n",
      "\n",
      "Worked on Machine Learning, Python and Computer Vision\n",
      "\n",
      "Decision Science Analytics Intern\n",
      "NiYO Solutions\n",
      "12/2019 - 06/2020, \n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "Bangalore, India\n",
      "\n",
      "Built  machine  learning  solutions  to  solve  important  business\n",
      "problems\n",
      "\n",
      "Worked with Data Scientists and Engineers to deploy solutions to\n",
      "a production environment\n",
      "\n",
      "Collaborated  with  business  leaders,  subject  matter  experts,  and\n",
      "decision  makers  to  develop  success  criteria  and  optimize  new\n",
      "products, features and models\n",
      "\n",
      "Communicated  key  results  to  senior  management  in  verbal,\n",
      "visual, and written media\n",
      "\n",
      "Engineer\n",
      "Ericsson Global India Pvt. Ltd.\n",
      "10/2016 - 04/2018, \n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "Noida, Uttar Pradesh\n",
      "\n",
      "Built  predictive  models  helpful  to  gain  business  insights  to  take\n",
      "important business decisions\n",
      "\n",
      "Applied ML techniques on the Telecom Customers data so to get\n",
      "a  deeper  understanding  of  the  customers  behavior  and  using\n",
      "those  insights  to  improvise  the  products  and  offerings  by  the\n",
      "company to its customers\n",
      "\n",
      "Worked on Python, ECMS, Machine Learning and SQL\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "M. Tech in Data Science and Business Analytics\n",
      "Narsee Monjee Institute of Management Studies\n",
      "(NMIMS)\n",
      "07/2018 - 07/2020, \n",
      "\n",
      "Mumbai, Maharashtra\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Instance  and  Semantic  Segmentation  Using  UNET\n",
      " (04/2021 - Present)\n",
      "\n",
      "The task is to categorize the multiple plants in the same image\n",
      "using Instance Segmentation\n",
      "Also,  Edge  detection  was  performed  on  Leaf  Edges  of  Basil  and\n",
      "Lettuce plants so as to get the plant area and the leaf count, this\n",
      "task  is  also  performed  using  UNET  as  it  was  giving  the  good\n",
      "results\n",
      "\n",
      "Basil, Lettuce and Hemp Image Classiﬁcation\n",
      " (03/2021 - 03/2021)\n",
      "\n",
      "Here the task was to identify which plant is in the image\n",
      "By  doing  so,  the  corresponding  pipeline  of  actions  will  be\n",
      "triggered  so  as  to  get  the  tasks  done,  this  is  also  a  part  of  the\n",
      "product being developed for the farmers\n",
      "This task is accomplished with 95% accuracy using Custom CNN\n",
      "Architecture\n",
      "\n",
      "Classiﬁcation and Regression Models on the Multiple\n",
      "Plants Data (02/2021 - 03/2021)\n",
      "\n",
      "Here  the  task  was  to  predict  plant  parameters  like  Stem  Girth,\n",
      "Plant  Height,  Fresh  Weight,  Dry  Weight  and  Leaf  Count  using\n",
      "previous data\n",
      "The  above  regression  tasks  were  achieved  successfully  with  an\n",
      "Adjusted R-Squared value of 0.98\n",
      "Another  task  was  to  classify  between  Basil  and  Lettuce  Classes\n",
      "based on the numerical data collected, this task was successfully\n",
      "done with an accuracy of 94% using Stacking\n",
      "\n",
      "Customer Churn Analysis for Salary Advance Product\n",
      "and Segmentation Model for Targeted Marketing\n",
      " (01/2020 - 04/2020)\n",
      "\n",
      "Objective  was  to  identify  factors  leading  to  Customer  Churning.\n",
      "The  aim  was  to  predict  whether  the  NiYO  Bharat  Customer  will\n",
      "churn  or  not  in  future,  the  task  was  accomplished  with  an\n",
      "accuracy of 91% using Random Forests\n",
      "The utility of Customer Segmentation project is for the Marketing\n",
      "Team who wanted to launch a Targeted and Marketing Campaign\n",
      "by  dividing  the  Customers  into  four  distinct  segments,  this  task\n",
      "was achieved with a good Silhouette Score of 0.87\n",
      "\n",
      "B. Tech in Computer Science and Engineering\n",
      "Amity University\n",
      "07/2012 - 07/2016, \n",
      "\n",
      "Greater Noida, Uttar Pradesh\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "TCS iON ProCert “Analytics” Certiﬁed\n",
      "\n",
      "Gold Badge for both Python and Sql on Hacker Rank\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Name using NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = extract_names(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rajat',\n",
       " 'Agarwaal Data Scientist Skilled Data Scientist',\n",
       " 'Machine Learning Engineer',\n",
       " 'Machine Learning Deep Learning Bangalore',\n",
       " 'Gitlab Tableau',\n",
       " 'Machine Learning',\n",
       " 'Python',\n",
       " 'Engineer Ericsson Global India Pvt',\n",
       " 'Uttar Pradesh Built',\n",
       " 'Applied ML',\n",
       " 'Machine Learning',\n",
       " 'Maharashtra PROJECTS Instance',\n",
       " 'Instance Segmentation',\n",
       " 'Basil',\n",
       " 'Lettuce',\n",
       " 'Hemp Image',\n",
       " 'Custom CNN Architecture',\n",
       " 'Stem Girth',\n",
       " 'Plant Height',\n",
       " 'Fresh Weight',\n",
       " 'Dry Weight',\n",
       " 'Basil',\n",
       " 'Lettuce Classes',\n",
       " 'Customer Churn Analysis',\n",
       " 'Salary Advance Product',\n",
       " 'Objective',\n",
       " 'Random',\n",
       " 'Marketing Campaign',\n",
       " 'Silhouette Score',\n",
       " 'Greater Noida',\n",
       " 'Uttar Pradesh',\n",
       " 'Python',\n",
       " 'Sql',\n",
       " 'Hacker Rank']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rajat Agarwaal'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0] + ' ' + names[1].split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_REG_IND = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "PHONE_REG_USA = re.compile(r'/^\\(?(\\d{3})\\)?[-]?(\\d{3})[-]?(\\d{4})$/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9958168687\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG_IND, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    " \n",
    "phone_number_ind = extract_phone_number(txt)\n",
    "print(phone_number_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG_USA, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        if resume_text.find(phone) >= 0:\n",
    "            return phone\n",
    "    return None\n",
    " \n",
    "phone_number_usa = extract_phone_number(txt)\n",
    "print(phone_number_usa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting EMAIL ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rajatagarwaal30@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "emails = extract_emails(txt)\n",
    "\n",
    "if emails:\n",
    "    print(emails)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Skills from the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'english',\n",
    "    'sql',\n",
    "    'deep learning',\n",
    "    'nlp',\n",
    "    'natural language processing',\n",
    "    'computer vision',\n",
    "    'pig',\n",
    "    'statistical analysis',\n",
    "    'gitlab',\n",
    "    'tableau'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 1, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "    \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "            \n",
    "#     print(found_skills)\n",
    "\n",
    "#     return found_skills\n",
    "    for skill in found_skills:\n",
    "        skill = skill.lower()\n",
    "        cnt = 0\n",
    "        for i in bigrams_trigrams:\n",
    "            i = i.lower()\n",
    "            if skill in i:\n",
    "                cnt += 1\n",
    "        print(skill.upper(), ' is repeated ' , cnt, ' times.')\n",
    "#     print(list(bigrams_trigrams))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE LEARNING  is repeated  15  times.\n",
      "PIG  is repeated  6  times.\n",
      "SQL  is repeated  18  times.\n",
      "COMPUTER VISION  is repeated  9  times.\n",
      "DATA SCIENCE  is repeated  3  times.\n",
      "GITLAB  is repeated  6  times.\n",
      "PYTHON  is repeated  24  times.\n",
      "MACHINE LEARNING  is repeated  15  times.\n",
      "NATURAL LANGUAGE PROCESSING  is repeated  1  times.\n",
      "DEEP LEARNING  is repeated  3  times.\n",
      "SQL  is repeated  18  times.\n",
      "STATISTICAL ANALYSIS  is repeated  3  times.\n",
      "TABLEAU  is repeated  6  times.\n"
     ]
    }
   ],
   "source": [
    "extract_skills(txt)\n",
    "\n",
    "# print(skills) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\rajat\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajat\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\rajat\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rajat\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\rajat\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def skill_exists(skill):\n",
    "    url = f'https://api.promptapi.com/skills?q={skill}&count=1'\n",
    "    headers = {'apikey': 'PH909DE8uJnBQEBCK2zAR2aFyqX0VZhm'}\n",
    "    response = requests.request('GET', url, headers=headers)\n",
    "    result = response.json()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return len(result) > 0 and result[0].lower() == skill.lower()\n",
    "    raise Exception(result.get('message'))\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if skill_exists(token.lower()):\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if skill_exists(ngram.lower()):\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "You have exceeded your daily/monthly API rate limit. Please review and upgrade your subscription plan at https://promptapi.com/subscriptions to continue.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-b4bc388c6ddc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskills\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskills\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-02c4a25d6f44>\u001b[0m in \u001b[0;36mextract_skills\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# we search for each token in our skills database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mskill_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mfound_skills\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-02c4a25d6f44>\u001b[0m in \u001b[0;36mskill_exists\u001b[1;34m(skill)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mskill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: You have exceeded your daily/monthly API rate limit. Please review and upgrade your subscription plan at https://promptapi.com/subscriptions to continue."
     ]
    }
   ],
   "source": [
    "skills = extract_skills(txt)\n",
    "\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "^\\(?([0-9]{3}\\)?[-]([0-9]{3})[-]([0-9]{4})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
