{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (20211012)\n",
      "Requirement already satisfied: cryptography in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from pdfminer.six) (35.0.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from pdfminer.six) (4.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from cryptography->pdfminer.six) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajat Agarwaal\n",
      "Data Scientist\n",
      "\n",
      "Skilled  Data  Scientist  with  2.5  years  of\n",
      "experience executing data driven solutions to\n",
      "increase  eﬃciency,  accuracy  and  utility  of\n",
      "internal  data  processing.  Experienced  at\n",
      "creating  regression  models,  classiﬁcation\n",
      "models  using  predictive  modelling,  Computer\n",
      "Vision  and  analyzing  data  mining  algorithms\n",
      "to  deliver  insights  and  implement  action\n",
      "oriented  solutions  to  complex  business\n",
      "problems.\n",
      "\n",
      "rajatagarwaal30@gmail.com\n",
      "\n",
      "09958168687\n",
      "\n",
      "Ghaziabad (NCR Region), India\n",
      "\n",
      "30 March, 1994\n",
      "\n",
      "linkedin.com/in/rajat-agarwaal\n",
      "\n",
      "WORK EXPERIENCE\n",
      "Machine Learning Engineer\n",
      "AgEYE Technologies\n",
      "01/2021 - Present, \n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Python\n",
      "\n",
      "C\n",
      "\n",
      "Machine Learning\n",
      "\n",
      "Deep Learning\n",
      "\n",
      "Bangalore, India\n",
      "\n",
      "Computer Vision\n",
      "\n",
      "Natural Language Processing\n",
      "\n",
      "Undertaking data collection, preprocessing and analysis\n",
      "\n",
      "SQL\n",
      "\n",
      "Pig\n",
      "\n",
      "Hive\n",
      "\n",
      "Basics of HDFS\n",
      "\n",
      "Building models to address business problems\n",
      "\n",
      "Statistical Analysis\n",
      "\n",
      "Gitlab\n",
      "\n",
      "Tableau\n",
      "\n",
      "Propose solutions and strategies to business challenges\n",
      "\n",
      "Worked on Machine Learning, Python and Computer\n",
      "Vision\n",
      "\n",
      "Decision Science Analytics Intern\n",
      "NiYO Solutions\n",
      "12/2019 - 06/2020, \n",
      "\n",
      "Bangalore, India\n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "Built  machine  learning  solutions  to  solve  important\n",
      "business problems\n",
      "\n",
      "Worked  with  Data  Scientists  and  Engineers  to  deploy\n",
      "solutions to a production environment\n",
      "\n",
      "Collaborated  with  business \n",
      "leaders,  subject  matter\n",
      "experts,  and  decision  makers  to  develop  success  criteria\n",
      "and optimize new products, features and models\n",
      "\n",
      "Communicated  key  results  to  senior  management  in\n",
      "verbal, visual, and written media\n",
      "\n",
      "Engineer\n",
      "Ericsson Global India Pvt. Ltd.\n",
      "10/2016 - 04/2018, \n",
      "\n",
      "Achievements/Tasks\n",
      "\n",
      "Noida, Uttar Pradesh\n",
      "\n",
      "Built  predictive  models  helpful  to  gain  business  insights\n",
      "to take important business decisions\n",
      "\n",
      "Applied  ML  techniques  on  the  Telecom  Customers  data\n",
      "so  to  get  a  deeper  understanding  of  the  customers\n",
      "behavior  and  using  those  insights  to  improvise  the\n",
      "products and oﬀerings by the company to its customers\n",
      "\n",
      "Worked on Python, ECMS, Machine Learning and SQL\n",
      "\n",
      "EDUCATION\n",
      "M. Tech in Data Science and Business\n",
      "Analytics\n",
      "Narsee Monjee Institute of Management\n",
      "Studies (NMIMS)\n",
      "07/2018 - 07/2020, \n",
      "\n",
      "Mumbai, Maharashtra\n",
      "\n",
      "B. Tech in Computer Science and\n",
      "Engineering\n",
      "Amity University\n",
      "07/2012 - 07/2016, \n",
      "\n",
      "Greater Noida, Uttar Pradesh\n",
      "\n",
      "PROJECTS\n",
      "Instance  and  Semantic  Segmentation  Using  UNET\n",
      " (04/2021 - Present)\n",
      "\n",
      "The task is to categorize the multiple plants in the same image\n",
      "using Instance Segmentation\n",
      "Also,  Edge  detection  was  performed  on  Leaf  Edges  of  Basil  and\n",
      "Lettuce  plants  so  as  to  get  the  plant  area  and  the  leaf  count,  this\n",
      "task is also performed using UNET as it was giving the good results\n",
      "\n",
      "Basil, Lettuce and Hemp Image Classiﬁcation\n",
      " (03/2021 - 03/2021)\n",
      "\n",
      "Here the task was to identify which plant is in the image\n",
      "By doing so, the corresponding pipeline of actions will be triggered\n",
      "so as to get the tasks done, this is also a part of the product being\n",
      "developed for the farmers\n",
      "This task is accomplished with 95% accuracy using Custom CNN\n",
      "Architecture\n",
      "\n",
      "Classiﬁcation and Regression Models on the Multiple\n",
      "Plants Data (02/2021 - 03/2021)\n",
      "\n",
      "Here the task was to predict plant parameters like Stem Girth, Plant\n",
      "Height,  Fresh  Weight,  Dry  Weight  and  Leaf  Count  using  previous\n",
      "data\n",
      "The  above  regression  tasks  were  achieved  successfully  with  an\n",
      "Adjusted R-Squared value of 0.98\n",
      "Another  task  was  to  classify  between  Basil  and  Lettuce  Classes\n",
      "based  on  the  numerical  data  collected,  this  task  was  successfully\n",
      "done with an accuracy of 94% using Stacking\n",
      "\n",
      "Customer Churn Analysis for Salary Advance Product\n",
      "and Segmentation Model for Targeted Marketing\n",
      " (01/2020 - 04/2020)\n",
      "\n",
      "Objective was to identify factors leading to Customer Churning. The\n",
      "aim was to predict whether the NiYO Bharat Customer will churn or\n",
      "not  in  future,  the  task  was  accomplished  with  an  accuracy  of  91%\n",
      "using Random Forests\n",
      "The  utility  of  Customer  Segmentation  project  is  for  the  Marketing\n",
      "Team who wanted to launch a Targeted and Marketing Campaign by\n",
      "dividing  the  Customers  into  four  distinct  segments,  this  task  was\n",
      "achieved with a good Silhouette Score of 0.87\n",
      "\n",
      "ACHIEVEMENTS\n",
      "TCS iON ProCert “Analytics” Certiﬁed\n",
      "\n",
      "Gold Badge for both Python and Sql on Hacker Rank\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/rajat/Downloads'\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('/home/rajat/Downloads/Rajat_Agarwaal_CV_Data_Scientist.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/rajat/snap/jupyter/6/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print(extract_text_from_docx('./resume.docx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess  \n",
    "import sys\n",
    "\n",
    "\n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  \n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    return (stdout.strip(), stderr.strip())\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     text, err = doc_to_text_catdoc('./resume-word.doc')\n",
    "\n",
    "#     if err:\n",
    "#         print(err)  # noqa: T001\n",
    "#         sys.exit(2)\n",
    "\n",
    "#     print(text)  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Names From Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/b8/09ac15436591cefc0adc882798d5cf629f13addae0495b20b682219e3afe/nltk-3.6.5-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/d5/0163eb0cfa0b673aa4fe1cd3ea9d8a81ea0f32e50807b0c295871e4aab2e/joblib-1.1.0-py2.py3-none-any.whl (306kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/e3/5bd531ce6dc4c08b41173576a377fff5e6e6ca87bb58d926acb2218e2b44/regex-2021.11.10-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (670kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from click->nltk) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/rajat/snap/jupyter/common/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.10.0.2)\n",
      "Installing collected packages: joblib, tqdm, regex, nltk\n",
      "Successfully installed joblib-1.1.0 nltk-3.6.5 regex-2021.11.10 tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1056)>\n",
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n",
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     text = extract_text_from_docx('resume.docx')\n",
    "#     names = extract_names(text)\n",
    "\n",
    "#     if names:\n",
    "#         print(names[0])  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting phone numbers from Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "\n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  # noqa: S607,S603\n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    return (stdout.strip(), stderr.strip())\n",
    "\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = doc_to_text_catdoc('resume.pdf')\n",
    "    phone_number = extract_phone_number(text)\n",
    "\n",
    "    print(phone_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting email addresses from Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('resume.pdf')\n",
    "    emails = extract_emails(text)\n",
    "\n",
    "    if emails:\n",
    "        print(emails[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting skills from the Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_docx('resume.docx')\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    print(skills) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
